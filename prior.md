---
layout: page
title: "Prior Work"
---

# *Effects of music on mood* 
<p>x</p>

# *Relationship between mood and music* 

<p>Researchers at Deezer worked on mood classification of music using hybrid techniques [1a]. The dataset they construct is an integral part of our project, as it 
gives context to the latent emotional space inhabited by music.</p>

<p>Hu et. al wrote about lyric text mining in music mood classification [2a], citing a "ceiling" to the mood classification that could be done just with frequency 
analysis. They also discuss the difficulties of creating ground truth datasets and go through considerable efforts to create their own. They distinguish between 
18 moods, much more granularity than in our project, and open us up to just how finely emotions can be discriminated when using text semantic analysis.</p>

<p>Laurier et. al also tackle the problem of mood classification of music through a multimodal approach [3a]. They compare classifications of songs using 
musical features and lyrics independently, then look into combining the two. In summary, they find that musical feature-based classification out-performs 
lyric-based classification, but combining the two modalities produces better results than any single approach.</p>

<p>Ã‡ano and Morisio split the VA space into the four quadrants that we base much of the project design on [4a]. They construct their own VA-embedded dataset 
based on song lyrics and achieve solid results of 74.25% classification accuracy (in the quadrant-based VA space partitioning).</p>

<p>Malheiro et. al incorporate new features to mood classification in songs. Namely, they introduce ideas of accounting for a slang dictionary (colloquial language 
that often won't appear in other dictionaries) and taking into account the structure of the song, interpreting lyrics differently depending on if they are perceived 
to be part of a song's verse or chorus. They were able to construct their own manually-annotated dataset of song lyrics labeled based on the VA model, and could 
achieve classification accuracy of 80.1% in quadrant-based VA classification.</p>



# *Emotion estimation from heart rate variability*

<p>x</p>

# *Systems that use mood to recommend music* 

<p>that one where you do a questionnaire and it's not real time. mani's former students' from 2006.</p> 

